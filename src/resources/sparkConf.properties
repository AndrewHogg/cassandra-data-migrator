spark.origin.host                                 localhost
spark.origin.username                             some-username
spark.origin.password                             some-secret-password
spark.origin.keyspaceTable                        test.a1

spark.target.scb                                  file:///aaa/bbb/secure-connect-enterprise.zip
spark.target.username                             client-id
spark.target.password                             client-secret
spark.target.keyspaceTable                        test.a2
spark.target.autocorrect.missing                  false
spark.target.autocorrect.mismatch                 false

spark.maxRetries                                  5
spark.readRateLimit                               20000
spark.writeRateLimit                              20000
spark.splitSize                                   10000
spark.batchSize                                   5

spark.query.origin                                partition-key,clustering-key,order-date,amount
spark.query.origin.partitionKey                   partition-key
spark.query.target                                partition-key,clustering-key,order-date,amount
spark.query.target.id                             partition-key,clustering-key
spark.query.types                                 9,1,4,3
spark.query.ttl.cols                              2,3
spark.query.writetime.cols                        2,3

################################## ENABLE ONLY IF IT IS A COUNTER TABLE #####################################
#spark.counterTable                                false
#spark.counterTable.cql
#spark.counterTable.cql.index                      0

######## ENABLE ONLY IF YOU WANT TO FILTER BASED ON WRITE-TIME (values must be in microseconds) #############
#spark.origin.writeTimeStampFilter                 false
#spark.origin.minWriteTimeStampFilter              0
#spark.origin.maxWriteTimeStampFilter              4102444800000000

############################### ONLY CHANGE IF YOU KNOW WHAT YOU ARE DOING ##################################
#spark.coveragePercent                             100
#spark.printStatsAfter                             100000
#spark.read.consistency.level                      LOCAL_QUORUM
#spark.read.fetch.sizeInRows                       1000
#spark.target.custom.writeTime                     0
#spark.fieldGuardraillimitMB                       10

#################### ONLY USE if count of recs greater than 10MB from Origin needed #########################
#spark.origin.checkTableforColSize                 false
#spark.origin.checkTableforColSize.cols            partition-key,clustering-key
#spark.origin.checkTableforColSize.cols.types      9,1

############################ ONLY USE if needing to filter data from Origin #################################
#spark.origin.FilterData                           false
#spark.origin.FilterColumn                         test
#spark.origin.FilterColumnIndex                    2
#spark.origin.FilterColumnType                     6%16
#spark.origin.FilterColumnValue                    test

########################## ONLY USE if SSL clientAuth is enabled on origin Cassandra/DSE ####################
#spark.origin.trustStore.path
#spark.origin.trustStore.password
#spark.origin.trustStore.type                     JKS
#spark.origin.keyStore.path
#spark.origin.keyStore.password
#spark.origin.enabledAlgorithms                   TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA

####################### ONLY USE if SSL clientAuth is enabled on target Cassandra/DSE #######################
#spark.target.trustStore.path
#spark.target.trustStore.password
#spark.target.trustStore.type                     JKS
#spark.target.keyStore.path
#spark.target.keyStore.password
#spark.target.enabledAlgorithms                   TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA

#############################################################################################################
# Following are the supported data types and their corresponding [Cassandra data-types]
#  0: ascii, text, varchar
#  1: int, smallint
#  2: bigint, counter
#  3: double
#  4: time, timestamp
#  5: map (separate type by %) - Example: 5%1%0 for map<int, text>
#  6: list (separate type by %) - Example: 6%0 for list<text>
#  7: blob
#  8: set (separate type by %) - Example: 8%0 for set<text>
#  9: uuid, timeuuid
# 10: boolean
# 11: tuple
# 12: float
# 13: tinyint
# 14: decimal
# 15: date
# 16: UDT [any user-defined-type created using 'CREATE TYPE']
# 17: varint

# Note: Ignore "Frozen" while mapping Collections (Map/List/Set) - Example: 5%1%0 for frozen<map<int, text>>
#
# "spark.query.ttl.cols" - Comma separated column indexes from "spark.query.origin" used to find largest TTL.
# "spark.query.writetime.cols" - Comma separated column indexes from "spark.query.origin" used to find largest writetime.
#  Note: The tool migrates TTL & Writetimes at row-level and not field-level.
#        Migration will use the largest TTL & Writetimes value per row.
#
# "spark.target.custom.writeTime" - User specified writetime. When set, this static value will be used as target writetime.
#
# Default value for "spark.origin.maxWriteTimeStampFilter" is "9223372036854775807" (max long value)
#
#############################################################################################################
