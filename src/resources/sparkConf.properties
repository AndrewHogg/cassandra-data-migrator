spark.origin.host                                 localhost
spark.origin.username                             some-username
spark.origin.password                             some-secret-password
spark.origin.keyspaceTable                        test.a1

spark.target.scb                                  file:///aaa/bbb/secure-connect-enterprise.zip
spark.target.username                             client-id
spark.target.password                             client-secret
spark.target.keyspaceTable                        test.a2
spark.target.autocorrect.missing                  false
spark.target.autocorrect.mismatch                 false

spark.maxRetries                                  3
spark.readRateLimit                               20000
spark.writeRateLimit                              20000
spark.splitSize                                   10000
spark.batchSize                                   5

spark.query.origin                                partition-key,clustering-key,order-date,amount
spark.query.origin.partitionKey                   partition-key
spark.query.target.id                             partition-key,clustering-key
spark.query.types                                 9,1,4,3
spark.query.ttl.cols                              2,3
spark.query.writetime.cols                        2,3

##### ENABLE ONLY IF COLUMN NAMES ON TARGET IS DIFFERENT FROM ORIGIN (SCHEMA & DATA-TYPES MUST BE SAME) #####
#spark.query.target                                partition-key,clustering-key,order-date,amount

################# ENABLE ONLY IF YOU WANT TO MIGRATE/VALIDATE SOME DATA BASED ON CQL FILTER #################
#spark.query.condition

################# ENABLE ONLY IF YOU WANT TO MIGRATE/VALIDATE SOME % (NOT 100%) DATA   ######################
#spark.coveragePercent                             10

#################### ENABLE ONLY IF WANT LOG STATS MORE OR LESS FREQUENTLY THAN DEFAULT #####################
#spark.printStatsAfter                             100000

################################# ENABLE ONLY IF IT IS A COUNTER TABLE ######################################
#spark.counterTable                                false
#spark.counterTable.cql
#spark.counterTable.cql.index                      0

######## ENABLE ONLY IF YOU WANT TO FILTER BASED ON WRITE-TIME (values must be in microseconds) #############
#spark.origin.writeTimeStampFilter                 false
#spark.origin.minWriteTimeStampFilter              0
#spark.origin.maxWriteTimeStampFilter              4102444800000000

######## ENABLE ONLY IF YOU WANT TO USE READ AND/OR WRITE CONSISTENCY OTHER THAN LOCAL_QUORUM  ##############
#spark.consistency.read                            LOCAL_QUORUM
#spark.consistency.write                           LOCAL_QUORUM

############# ENABLE ONLY IF YOU WANT TO REDUCE FETCH-SIZE TO AVOID FrameTooLongException  ##################
#spark.read.fetch.sizeInRows                       1000

############### ENABLE ONLY IF YOU WANT TO USE CUSTOM FIXED WRITETIME VALUE ON TARGET  ######################
#spark.target.custom.writeTime                     0

#################### ONLY USE if SKIPPING recs greater than 10MB from Origin needed #########################
#spark.fieldGuardraillimitMB                       10

#################### ONLY USE if count of recs greater than 10MB from Origin needed #########################
#spark.origin.checkTableforColSize                 false
#spark.origin.checkTableforColSize.cols            partition-key,clustering-key
#spark.origin.checkTableforColSize.cols.types      9,1

############################ ONLY USE if needing to filter data from Origin #################################
#spark.origin.FilterData                           false
#spark.origin.FilterColumn                         test
#spark.origin.FilterColumnIndex                    2
#spark.origin.FilterColumnType                     6%16
#spark.origin.FilterColumnValue                    test

########################## ONLY USE if SSL clientAuth is enabled on origin Cassandra/DSE ####################
#spark.origin.trustStore.path
#spark.origin.trustStore.password
#spark.origin.trustStore.type                     JKS
#spark.origin.keyStore.path
#spark.origin.keyStore.password
#spark.origin.enabledAlgorithms                   TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA

####################### ONLY USE if SSL clientAuth is enabled on target Cassandra/DSE #######################
#spark.target.trustStore.path
#spark.target.trustStore.password
#spark.target.trustStore.type                     JKS
#spark.target.keyStore.path
#spark.target.keyStore.password
#spark.target.enabledAlgorithms                   TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA

#############################################################################################################
# Following are the supported data types and their corresponding [Cassandra data-types]
#  0: ascii, text, varchar
#  1: int
#  2: bigint, counter
#  3: double
#  4: timestamp
#  5: map (separate type by %) - Example: 5%1%0 for map<int, text>
#  6: list (separate type by %) - Example: 6%0 for list<text>
#  7: blob
#  8: set (separate type by %) - Example: 8%0 for set<text>
#  9: uuid, timeuuid
# 10: boolean
# 11: tuple
# 12: float
# 13: tinyint
# 14: decimal
# 15: date
# 16: UDT [any user-defined-type created using 'CREATE TYPE']
# 17: varint
# 18: time
# 19: smallint

# Note: Ignore "Frozen" while mapping Collections (Map/List/Set) - Example: 5%1%0 for frozen<map<int, text>>
#
# "spark.query.ttl.cols" - Comma separated column indexes from "spark.query.origin" used to find largest TTL.
# "spark.query.writetime.cols" - Comma separated column indexes from "spark.query.origin" used to find largest writetime.
#  Note: The tool migrates TTL & Writetimes at row-level and not field-level.
#        Migration will use the largest TTL & Writetimes value per row.
#
#############################################################################################################